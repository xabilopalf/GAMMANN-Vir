---
title: "GAM-ANNstyle"
author: "Xabier López Alforja"
date: "`r format(Sys.time(), '%d %B, %Y')`"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#LOADING/INSTALLING REQUIRED PACKAGES
required = c(# 1. generally needed packages (and "vegan" for Mantel-test)
              "readr", "readxl", "vegan", "geosphere", "tidyverse", "tibble", "ggplot2", "corrplot","ggcorrplot",
             # 2. Packages for the GAM modelling part
             "mgcv","ggplot2","itsadug","data.table","car","grid","lubridate", "Amelia", "VIM", "visdat", "naniar",
             # 3. Packages for the ANN part 
             "psych","scales", "foreach", "doParallel", "NeuralNetTools", "nnet", "GGally", "Metrics", "caret", "dplyr","rje", "gridExtra","devtools")

# this is a loop to install and load the packages required for the analysis
for (i in required) {
  if (!require(i, character.only = T)) {
    install.packages(i, character.only = T)
  }
  library(i, character.only = T)
}

source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r') #this is a function we will be needing for plotting the neural networks
 
```
# Title 

**Predicting Viral Abundance Time Series through the use of Artificial Neural Networks (ANNs)**

# Abstract

```         
The present document is part of a larger project that aims to analyze the temporal patterns of microorganisms, including viruses, in a 20-year time series. The project is divided into three main parts: (1) Anomaly analyses, (2) Generalized Additive Models (GAMs), and (3) Artificial Neural Networks (ANNs). This document focuses on the third part, which aims to predict the viral abundance time series through the use of ANNs. The document is structured as follows: (1) Anomaly analyses, (2) GAMs: Generalized Additive Models, (3) Machine Learning: Predicting Viral Abundance Time Series through the use of Artificial Neural Networks (ANNs), and (4) References.
```

# Research Questions:

**1. Are there significant temporal patterns in the abundance, activity, and diversity of microorganisms, including viruses, in the 20-year time series?**

-   Explore temporal variations in microorganisms, especially viruses, to identify possible seasonal patterns or anomalous events.

**2. Is there a significant relationship between virus abundance and changes in abiotic variables such as temperature, salinity, and turbidity during the time series? What are the most relevant variables affecting virus abundance according to the models?**

-   Investigate how seasonality and trend components contribute to the variability in each analyzed variable using GAMs and ANNs.

**3. Are there significant seasonal or yearly variations in virus abundance, and are these variations associated with changes in climatic conditions over time?**

-   Analyze whether virus abundance exhibits seasonal or yearly patterns, and if these patterns are linked to changes in climatic conditions, especially in the context of climate change.

# Hypotheses:

**1. Temporal Hypothesis (variability/stability):**

```         
- H0: There are no significant temporal patterns in the 20-year time series.

- H1: The time series exhibits significant seasonal patterns, long-term trends, or anomalous events.
```

**2. Hypothesis regarding Variables Relevant to Virus Abundance (biotic/abiotic independence/dependence):**

```         
- H0: There are no specific variables that stand out as significantly influential in virus abundance.

- H1: Specific variables, identified through partial GAMs and ANNs models, exist that stand out in their impact on virus abundance.
```

**3. Climate Invariability Hypothesis (stability/association):**

```         
- H0: There is no significant response of viruses to climate change, demonstrated by the lack of significant seasonal or yearly variations in their abundance during the time series.

- H1: There is a response of viruses to climate change, manifested by seasonal or yearly variations in their abundance, and a significant association with climatic conditions over time is established, supported by pre and post-2012 analyses.
```

# 1. Anomaly analyses

```{r, warning=FALSE}
anom.data <- read_excel("DADES_DV_tot.xlsx", sheet = "Anomalies_for_R", col_types = c("numeric", "text", "text", "numeric", "numeric", 
               "numeric", "numeric", "numeric", "numeric", "numeric",
               "numeric", "numeric", "numeric", "numeric", "numeric",
               "numeric", "numeric", "numeric", "numeric", "numeric",
               "numeric", "numeric", "numeric", "numeric"))
```

This dataset contains the anomalies for the 24 variables (columns) and 20 years (rows). The first three columns are the year, the month and the sample ID. The rest of the columns are the anomalies for the 24 variables.

With the following code we will generate a graph for each variable, showing the anomalies for each year.

```{r AnomalyGraphics, warning=FALSE}
# Create a grouping by year
means_df <- anom.data %>% 
  group_by(Year) %>% 
  summarise_all(mean, na.rm = TRUE) # Calculate the means for each column
means_df <- as.data.frame(means_df)
theme_set(theme_bw())

#If you want to see any graph in particular, run the following code
ggplot(means_df, aes(x=Year, y=means_df[,21])) +
    geom_bar(stat="identity", fill="#1D7874") +
    geom_smooth(method = "lm" , se = FALSE, col = "#ffc701") +
    labs(x="Year", y= "Anomaly") +
    theme(text = element_text(size = 30)) +
    ggtitle(paste("Anomalies", colnames(means_df)[21]))
```

The following command will generate a graph for each column (variable) and save it in separated files.

```{r eval=FALSE, warning=FALSE, include=TRUE}
# Generate a graph for each column and save it in separated files
theme_set(theme_bw())
for (i in 4:ncol(means_df)) {
  ggplot(means_df, aes(x=Year, y=means_df[,i])) +
    geom_bar(stat="identity", fill="#1D7874") +
    geom_smooth(method = "lm" , se = FALSE, col = "#ffc701") +
    labs(x="Year", y= "Anomaly") +
    theme(text = element_text(size = 30)) +
    ggtitle(paste("Anomalies", colnames(means_df)[i]))
  
  ggsave(paste("anomalies_", colnames(means_df)[i], ".svg"), width=11, height=6)
}
```

```{r GRAFICAS DE SERIES TEMPORALES, eval=FALSE, include=FALSE}

data <- read_excel("~/Desktop/FPI_Doctorado/04_Active_Projects/05_TimeSeries_RAPDs/Time_Series_and_RAPDs/DADES_DV_tot.xlsx", 
                   sheet = "R_version_table", col_types = c("numeric", 
                                         "text", "text", "text", "numeric", "numeric", 
                                         "numeric", "numeric", "numeric", "numeric", "numeric",
                                         "numeric", "numeric", "numeric", "numeric", "numeric",
                                         "numeric", "numeric", "numeric", "numeric", "numeric",
                                         "numeric", "numeric", "numeric", "numeric", "numeric"))


df.plots<- data %>% dplyr::select(1:14 ,16:19 , 22, 26:28)
lista<- data %>% dplyr::select(1:14 ,16:19 , 22,25)
all_boxplots <- function(df.plots){
  for (i in 1:ncol(lista)) {
    ggplot() + 
        #gneom_point(data = df, aes(colour = Year, x = Month, y = Temperature), alpha = 0.7, shape = 19, size = 4)+
        geom_jitter(data = df.plots, aes(colour = Year, x = Month, y = df.plots[,i]), alpha = 0.9, shape = 19, size = 4)+
        #geom_errorbar(data = stat.df, aes(x = Month,ymin = Temperature.mean-Temperature.sd, ymax = Temperature.mean+Temperature.sd ), width = 0.2)+
          geom_boxplot(data = df.plots, aes(x = Month, y = df.plots[,i]), width = 0.5, alpha = 0.8, color = "#25326c")+
        #geom_violin(data = df, aes(x = Month, y = Temperature), width = 0.5,alpha = 0.5)+
        labs(y = colnames(df.plots[,i]), x = "Month") +
        theme( axis.text.x = element_text(face = "bold",colour = "black", size = 20, angle = 45, hjust = 1, vjust = 1), 
               axis.text.y = element_text(face = "bold", size = 11, colour = "black"), 
               axis.title= element_text(face = "bold", size = 14, colour = "black"), 
               panel.background = element_blank(), 
               panel.border = element_rect(fill = NA, colour = "black"), 
               legend.title = element_text(size =12, face = "bold", colour = "black"),
               legend.text = element_text(size = 10, face = "bold", colour = "black")) +
        #geom_line(data = df,aes( y = Temperature, x = Month), size= 1)+
        scale_colour_continuous(high = "#25326c", low = "#f9c800") +
        scale_x_discrete(limits = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")) +
  #ylim(0.00, 0.35) + # Limitar el eje Y 
    ggsave(paste0("boxplot_2_", colnames(df.plots)[i], ".svg"), width=11, height=6)
  }
}
  
all_boxplots(df.plots)



############# el siguiente si funciona en loop ############
library(dplyr)
library(ggplot2)

df.plots <- df.plots %>%
  select(1:2, 5:20)
lista <- data %>%
  select(5:25)

df.plots<- data %>% dplyr::select(1:14, 16:19 , 22, 25)  
df.plots<- as.data.frame(df.plots)
lista<- data %>% dplyr::select(7:16 ,18:21 , 24, 27)

all_boxplots <- function(df.plots) {
  for (i in 1:ncol(lista)) {
    col_name <- names(df.plots)[i]
    p <- ggplot() +
      geom_point(
        data = df.plots,
        aes(colour = Year, x = Month, y = !!sym(col_name)), # Utiliza !!sym() para referenciar la columna por nombre
        alpha = 0.9,
        shape = 19,
        size = 3
      ) +
      geom_boxplot(
        data = df.plots,
        aes(x = Month, y = !!sym(col_name)), # Utiliza !!sym() para referenciar la columna por nombre
        width = 0.5,
        alpha = 0.0,
        outlier.shape=NA,
        color = "#25326c"
      ) +
  theme_minimal()+
      labs(y = col_name, x = "Month") +
      theme(
        axis.text.x = element_text(
          face = "bold",
          colour = "black",
          size = 13,
          angle = 45,
          hjust = 1,
          vjust = 1
        ),
        axis.text.y = element_text(face = "bold", size = 11, colour = "black"),
        axis.title = element_text(face = "bold", size = 13, colour = "black"),
        panel.background = element_blank(),
        panel.border = element_rect(fill = NA, colour = "black"),
        legend.title = element_text(size = 12, face = "bold", colour = "black"),
        legend.text = element_text(size = 10, face = "bold", colour = "black")
      ) +
      scale_colour_continuous(high = "#071d6b", low = "#00d3b9") +
      scale_x_discrete(
        limits = c(
          "Jan",
          "Feb",
          "Mar",
          "Apr",
          "May",
          "Jun",
          "Jul",
          "Aug",
          "Sep",
          "Oct",
          "Nov",
          "Dec"
        )
      )
    ggsave(paste0("boxplot_2_", col_name, ".svg"), plot = p, width = 11, height = 6)
  }
}


setwd("~/Downloads/")

all_boxplots(df.plots)


#############################################################



 p=ggplot() + 
        geom_point(data = df.plots, aes(colour = Year, x = Month, y = AbunBact), alpha = 0.8, shape = 19, size = 3)+
        #geom_jitter(data = df.plots, aes(colour = Year, x = Month, y = df.plots$PO4), alpha = 0.9, shape = 19, size = 4)+
          geom_boxplot(data = df.plots, aes(x = Month, y = AbunBact), width = 0.5, alpha = 0.0, outlier.shape=NA, color = "#071d6b")  +
  theme_minimal()+
        labs(y = "AbunBact", x = "Month") + #colnames(df.plots$PO4)
        theme( axis.text.x = element_text(face = "bold",colour = "black", size = 13
                                          , angle = 45, hjust = 1, vjust = 1), 
               axis.text.y = element_text(face = "bold", size = 11, colour = "black"), 
               axis.title= element_text(face = "bold", size = 13, colour = "black"), 
               panel.background = element_blank(), 
               panel.border = element_rect(fill = NA, colour = "black"), 
               legend.title = element_text(size =12, face = "bold", colour = "black"),
               legend.text = element_text(size = 10, face = "bold", colour = "black")) +
        scale_colour_continuous(high = "#071d6b", low = "#00d3b9") +
        scale_x_discrete(limits = c("Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")) + ylim(0.00, 4e+06)
  #+ limits = c(0, 3.5e+07) ,labels = function(var) format(var, scientific = TRUE) #Viral Abundance

 ggsave(paste0("boxplot_", "AbunBact", ".svg"),plot=p, width=11, height=6)

```

# 2. GAMs: Generalized Additive Models

**Why Use GAMs?**

1.  The relationship between predictor variables (i.e., independent) and the response variable (i.e., dependent) does **not need to be linear**.

2.  We **do not need** to know the **functional form of the relationship** in advance.

3.  They are highly flexible models that allow the interpretation (graphically) of partial effects of each independent variable.

4.  We can:

    ```         
     - Include categorical predictors and interactions.

     - Use distributions other than normal for the dependent variable.

     - Include correlations between observations (e.g., repeated measures, nested designs) - mixed models.
    ```

```{r, warning=FALSE}
#LOADING THE DATA
data <- read_excel("DADES_DV_tot.xlsx", sheet = "R_augmented_table", col_types = c("numeric", "text", "text", "text", "date", "text", 
              "numeric", "numeric", "numeric", "numeric", "numeric",
              "numeric", "numeric", "numeric", "numeric", "numeric",
              "numeric", "numeric", "numeric", "numeric", "numeric",
              "numeric", "numeric", "numeric", "numeric", "numeric",
              "numeric","numeric"))

df<- data %>% dplyr::select(1:16 ,18:21 , 24) # quitamos las variables que estaban correlacionadas entre si (porque habia variables que componían otras)
```

## 2.1. Checking the missing data in the dataset

We want to check the missing data in the dataset and remove the rows with COMPLETE missing data, because there were no values of any variables studied (for example, the COVID-19 pandemic lockdown months). We will also check the missing data proportion over variables.

```{r warning=FALSE, include=FALSE}
df_gam<- data %>% dplyr::select(1, 5:27)  
missmap(df_gam) #from Amelia package

# Índices de filas que deseas eliminar
indices_a_eliminar <- c(1, 2, 7, 11, 25, 276, 277, 282, 311)

# Eliminar las filas con los índices especificados
df_gam <- df_gam %>% slice(-indices_a_eliminar)

#see our missing data proportion over variables
aggr_plot <- aggr(df_gam, col=c('navyblue','red'), numbers=TRUE, sortVars=TRUE, labels=names(df), cex.axis=.5, gap=3, ylab=c("Histogram of missing data","Pattern")) #from VIM package
vis_miss(df_gam, show_perc = F) + coord_flip() #from naniar package

missmap(df_gam)
```

## 2.2. From Dates to Months and Days

We need to transform the date variable to a date format and generate the variables "Month" and "Dias" (days) to use them in the GAM model. We will also remove the "TIME" variable, which is not necessary for the analysis.

```{r}

attach(df_gam)
df_gam$DATE <- as.Date(df_gam$DATE, "%Y/%m/%d") # formato de fecha en inglés
df_gam$Month <- as.numeric(format(df_gam$DATE,'%m')) # generar variable Mes
df_gam$Dia1 <- rep(df_gam$DATE[1], nrow(df_gam)) # fecha inicial de la serie
df_gam$Dias <- (interval(df_gam$Dia1, df_gam$DATE) %/% days(1))+1 # generar variable Dias
df_gam$Dia1 <- NULL
df_gam$TIME <- NULL

head(df_gam)
```

## 2.3. Modelling the Viral Abundance GAM

We will use the "gamm" function from the "mgcv" package to fit a GAM model to the viral abundance data. We will use a combination of cyclic cubic regression splines for the Month variable and cubic regression splines for the Days variable. We will also include a tensor product interaction term between Month and Days to account for the interaction between these two variables.

Our data also show correlation over time, indicating that measurements at one moment are related to measurements at earlier moments. To account for this autocorrelation, they are using a mixed model called "gamm()" and incorporating a continuous first-order autoregressive structure (corCAR1) into the model. This means they are considering how data from one day is related to data from previous days.

```{r}
m_vir<-gamm(AbunVir ~ s(Month, bs="cc") + s(Dias, bs="cr", k=20) + ti(Month,Dias, bs=c("cc","cr")), family=quasipoisson , correlation = corCAR1(form = ~ Dias), data = df_gam)

plot(m_vir$gam, scale=0, scheme=1, pages=1)

summary(m_vir$gam)

par(mfrow=c(2,2))
gam.check(m_vir$gam, type="pearson")

par(mfrow=c(1,1))
vis.gam(m_vir$gam, type="response", 
        view=c("Month","Dias"), main="Viral Abundance",
        plot.type = "contour", contour.col="black",color="terrain")
points(df_gam$Month,df_gam$Dias,pch = 20) #agregar puntos de muestreo
```

## 2.4. Interpreting the results of the GAM model

Understanding gamcheck() results: The gam.check() function is used to check the model assumptions. The function provides a series of diagnostic plots that can be used to assess the model fit.

**Model statistics** Here are some possible interpretations:

-   **Non-significant result:** A non-significant result is generally a good sign. It indicates that, based on Pearson correlation, the model seems to fit well to the non-linear structure present in the data.

-   **Significant result:** If the result is significant, it could suggest that there are additional non-linear patterns in the data that are not being captured by the model. This might imply the need to adjust the model more flexibly or consider additional terms.

However, the following scenario should also be considered:

When fitting a model with 'gamm', be cautious in interpreting the results, as residuals-based checks can be deceptive. Specifically, pay attention to interpretation when using basis dimensions (k). A low p-value (index k \< 1) could indicate that k is too low, especially if effective degrees of freedom (edf) are close to k. In summary, make sure to properly assess the choice of k in your GAM model, as a low k index could be problematic.

**Model diagnostics with graphs**

The following plots are generated:

-   **Residuals vs Fitted:** This plot is used to check for non-linearity in the model. The residuals should be randomly scattered around the horizontal line at zero. If the residuals show a pattern, it suggests that the model is not capturing the non-linear structure in the data.

-   **QQ plot of residuals:** This plot is used to check for the normality of the residuals. The points should fall approximately along the diagonal line. If the points deviate from the line, it suggests that the residuals are not normally distributed.

-   **Scale-Location plot:** This plot is used to check for homoscedasticity. The residuals should be randomly scattered around the horizontal line. If the residuals show a pattern, it suggests that the variance of the residuals is not constant across the range of fitted values.

-   **Residuals vs Leverage:** This plot is used to check for influential cases. The plot shows the Cook's distance for each observation. Observations with a Cook's distance greater than 1 are considered influential.

## 2.4. Partial GAM for understanding virus-environment relationships

### 2.4.1. Partial GAM for Viral Abundance vs Nutrients

```{r}
mod_nutr<-gamm(AbunVir ~ s(SiO4, bs = "cr") + s(NH4, bs = "cr") + s(PO4, 
    bs = "cr") + s(NO2, bs = "cr") + s(NO3, bs = "cr") , 
              correlation = corCAR1(form = ~ Dias) , family=quasipoisson , data = df_gam)
plot(mod_nutr$gam, scale=0, scheme=1, pages=1)
summary(mod_nutr$gam)

par(mfrow=c(2,2))
gam.check(mod_nutr$gam, type="pearson")
```

### 2.4.2. Partial GAM for Viral Abundance vs host

```{r}
mod_host<-gamm(AbunVir~   s(AbunBact, bs="cr") + s(Abun_HNF, bs="cr")+  s(Abun_PNF, bs="cr")+  s(Synechococcus, bs="cr")+  s(Prochlorococcus, bs="cr"), family=quasipoisson , data = df_gam)

plot(mod_host$gam, scale=0, scheme=1, pages=1)
summary(mod_host$gam)

par(mfrow=c(2,2))
gam.check(mod_host$gam, type="pearson")
```

### 2.4.3. Partial GAM for Viral Abundance vs env.variables

```{r}
mod_var_fq<-gamm(AbunVir~   s(Temperature, bs="cr") + s(Secchi_Disk, bs="cr")+  s(Salinity, bs="cr"), family=quasipoisson , data = df_gam)

plot(mod_var_fq$gam, scale=0, scheme=1, pages=1)
summary(mod_var_fq$gam)

par(mfrow=c(2,2))
gam.check(mod_var_fq$gam, type="pearson")
```

## 2.5. GAM for Viral Abundance vs. Inflection Point

We will use the inflection point to divide the time series into two periods: pre-2012 and post-2012. We will then fit a GAM model to the viral abundance data, including the inflection point as a predictor variable. We will use a combination of cyclic cubic regression splines for the Month variable and cubic regression splines for the Days variable. We will also include a tensor product interaction term between Month and Days to account for the interaction between these two variables.

```{r}
attach(df_gam) 

df_gam$inflection <- ifelse(df_gam$Dias <= 4000, "pre", "post")
df_gam$inflection <- as.factor(df$inflection)

##### ESTO SIRVE PARA ARGUMENTAR QUE LOS MODELOS SON MEJORES SI TENEMOS EN CUENTA LA INFLECTION POINT #####
m1<-gamm(AbunVir ~ s(Month, bs="cc") + s(Dias, bs="cr"), family=quasipoisson , correlation = corCAR1(form = ~ Dias), data = df_gam)

m2<-gamm(AbunVir ~ inflection + s(Month, by=inflection, bs="cc") + s(Dias, bs="cr"), family=quasipoisson , correlation = corCAR1(form = ~ Dias), data = df_gam)

# AIC del modelo con y sin inflection point
AIC(m1$lme, m2$lme) 

plot(m2$gam, scale=0, scheme=1, pages=1) #  plot the model
summary(m2$gam)
par(mfrow=c(2,2))
gam.check(m2$gam, type="pearson")

par(mfrow=c(1,1))
plot_smooth(m2$gam, view = "Month",
  plot_all = "inflection",
  hide.label=TRUE,rm.ranef = F, 
  rug=F, ylab="Abundancia Viral")
```

## 2.6. References

-   [Capitulo GAMs libro Victoria Quiroga](https://limno-con-r.github.io/libro/gam.html)
-   [Curso de GAMs en R](https://noamross.github.io/gams-in-r-course/)
-   [Documentación oficial del paquete 'gam'](https://cran.r-project.org/web/packages/gam/gam.pdf)

# 3. Machine Learning: Predicting Viral Abundance Time Series through the use of Artificial Neural Networks (ANNs)

Ensure the number of explanatory variables is less than the number of objects (sites, samples, observations etc.) in your data matrices. If not your system is [overdetermined](https://sites.google.com/site/mb3gustame/warnings/warning-overdetermination).

```{r, warning=FALSE}
#LOADING THE DATA
data <- read_excel("DADES_DV_tot.xlsx", sheet = "R_augmented_table", col_types = c("numeric", "text", "text", "text", "numeric", "numeric", 
              "numeric", "numeric", "numeric", "numeric", "numeric",
              "numeric", "numeric", "numeric", "numeric", "numeric",
              "numeric", "numeric", "numeric", "numeric", "numeric",
              "numeric", "numeric", "numeric", "numeric", "numeric",
              "numeric","numeric"))

df<- data %>% dplyr::select(1:16 ,18:21 , 24) # quitamos las variables que estaban correlacionadas entre si (porque habia variables que componían otras)

```

## 3.1. Checking missing data in the dataset

Here we will not only be cleaning the data from NAs or non complete samples, but also we will determine which variables we will use, based on the previous chapter's Mantel-Test:

```{r warning=FALSE, include=FALSE}
df.1<- df %>% dplyr::select(7:19)  # quitamos las variables que estaban correlacionadas entre si (porque habia variables que componían otras)
#df.1<- dplyr::select(df, "AbunVir", "Salinity", "PO4", "Secchi_Disk", "CHL", "AbunBact", "Abun_HNF")
df.1<-df.1[complete.cases(df.1),] #only cases without NAs
```

## 3.2. Linearity check: corrplots

```{r}
testRes = cor.mtest(df.1, conf.level = 0.95)
corrplot(cor(df.1), addCoef.col = 'black', type = 'lower', p.mat = testRes$p, sig.level = 0.05, title= "sig.level = 0.05") #check Highly correlated variables corr > 0.8. This can be a problem in some machine learning scenarios ## specialized the insignificant value according to the significant level
corrplot(cor(df.1), addCoef.col = 'black', type = 'lower', p.mat = testRes$p, sig.level = 0.05, tl.col = "black", tl.srt = 90, tl.cex = 0.8, addCoef.size = 2, number.cex = 0.7, cl.pos = "r", title = "sig.level = 0.05" )

pairs.panels(df.1, method="pearson",ellipses=FALSE,
             gap = 0, hist.col="#ffc701", col="lightblue", stars = T,
             pch=21, main= "Correlation plot: Pearson Test") #(calcula los coeficientes con pearson)

ggcorrplot(cor(df.1, method = "pearson"), 
           hc.order = TRUE,
           p.mat = testRes$p,
           ggtheme = ggplot2::theme_minimal,
           type = "lower", 
           colors = c("#26306e", "white", "#ffc701"),
           pch = 4, 
           pch.cex = 8,
           lab = TRUE,
           title = "Correlation plot with scatter plots"
)
```

## 3.3. Hyperparameter tunning

Finding the optimal tuning parameters for a machine learning problem can often be very difficult. We may encounter **overfitting,** which means our machine learning model trains too specifically on our training dataset and causes higher levels of error when applied to our test/holdout datasets. Or, we may run into **underfitting,** which means our model doesn't train specifically enough to our training dataset. This also leads to higher levels of error when applied to test/holdout datasets.

Tuning hyperparameters manually means more control over the process. If you are researching or studying tuning and how it affects the network weights then doing it manually would make sense. However, manual tuning is a tedious process since there can be many trials and keeping track can prove costly and time-consuming. This isn't a very practical approach when there are a lot of hyperparameters to consider.

#### 3.3.1. Number of neurons needed for my ANN

```{r Checking the number of neurons needed for my ANN}
set.seed(seed_num)
df_neuron_check<- data.frame(hidden = numeric(), PCC = numeric(), RMSE = numeric())
for (i in 1:20) {
  print(paste("number of hidden layers:", i))
  trained_net<-nnet(AbunVir ~. ,data=df.1,size=i,linout=TRUE,maxit=1500)
  pearson_cor<-cor.test(df.3$AbunVir,trained_net$fitted.values,method="pearson") 
  rmse<-rmse(df.3$AbunVir,trained_net$fitted.values)
  df_neuron_check[i,] <- c(i, pearson_cor$estimate, rmse)
}

ggplot(data = df_neuron_check, aes(x = hidden, y = PCC)) + 
  geom_line(aes(color = "PCC")) +
  geom_line(aes(y = RMSE, color = "RMSE")) +
  geom_hline(yintercept=0.8, linetype = "dashed" ) +
  geom_vline(xintercept=4, linetype = "dashed" ) +
  scale_y_continuous(sec.axis = sec_axis(~., name = "RMSE")) +
  scale_color_manual(name = "Parametres", values = c("PCC" = "#26306e", "RMSE" = "#ffc701")) +
  ggtitle(paste("Checking the number of neurons needed for my ANN"))
```

#### 3.3.2. Number of iterations needed for my ANN

```{r Checking the number of iterations needed for my ANN}
set.seed(9999)
df_maxit_check<- data.frame(iterations = numeric(), PCC = numeric(), RMSE = numeric())

maxit_vec <- c(300, 500, 1000, 1500, 2000, 3000, 5000, 7000, 10000)

for (i in 1:length(maxit_vec)) {
  print(paste("number of iterations:", maxit_vec[i]))
  trained_net<-nnet(AbunVir ~. ,data=df.3,size=5,linout=TRUE,maxit=maxit_vec[i])
  pearson_cor<-cor.test(df.3$AbunVir,trained_net$fitted.values,method="pearson") 
  rmse<-rmse(df.3$AbunVir,trained_net$fitted.values)
  df_maxit_check[i,] <- c(maxit_vec[i], pearson_cor$estimate, rmse)
}

ggplot(data = df_maxit_check, aes(x = iterations, y = PCC)) + 
  geom_line(aes(color = "PCC")) +
  geom_line(aes(y = RMSE, color = "RMSE")) +
  geom_vline(xintercept=1500, linetype = "dashed" ) +
  scale_y_continuous(sec.axis = sec_axis(~., name = "RMSE")) +
  scale_color_manual(name = "Parametres", values = c("PCC" = "#26306e", "RMSE" = "#ffc701")) +
  ggtitle(paste("Checking the number of iterations needed for my ANN"))
```

#### 3.3.3. Different starting weights needed for my ANN

```{r Checking the different starting weights needed for my ANN}
df_startweights_check<- data.frame(combination= numeric(), starting_weights = numeric(), PCC = numeric(), RMSE = numeric())

starting_weights_list <- runif(10, min = -1, max = 1)

for (i in 1:length(starting_weights_list)) {
  print(paste("Starting weights: ", starting_weights_list[[i]]))
  trained_net<-nnet(AbunVir ~. ,data=df.3,size=3,linout=TRUE,maxit=1500, startweights = starting_weights_list[[i]])
  pearson_cor<-cor.test(df.3$AbunVir,trained_net$fitted.values,method="pearson") 
  rmse<-rmse(df.3$AbunVir,trained_net$fitted.values)
  df_startweights_check[i,] <- c(i, starting_weights_list[[i]], pearson_cor$estimate, rmse)
}

ggplot(data = df_startweights_check, aes(x = combination, y = PCC)) + 
  geom_line(aes(color = "PCC")) +
  geom_line(aes(y = RMSE, color = "RMSE")) +
  #geom_hline(yintercept=0.8396903, linetype = "dashed" ) +
  #geom_vline(xintercept=1500, linetype = "dashed" ) +
  scale_y_continuous(sec.axis = sec_axis(~., name = "RMSE")) +
  scale_color_manual(name = "Parametres", values = c("PCC" = "#26306e", "RMSE" = "#ffc701")) +
  ggtitle(paste("Checking the different starting weights needed for my ANN"))
```

#### 3.3.4. Fraction used as a training set in my ANN

```{r Checking the partition fraction used as a training set in my ANN}
df_sample_ammount_check<- data.frame(iteration= numeric(), Fraction = numeric(), PCC = numeric(), RMSE = numeric())

partition_size <- c(0.70, 0.75, 0.8, 0.85)

for (i in 1:length(partition_size)) {
  print(paste("Starting weights: ", partition_size[[i]]))
  parts<-createDataPartition(df.3$AbunVir,p=partition_size[[i]],list=FALSE)
  training<-df.3[parts,] 
  trained_net<-nnet(AbunVir ~. ,data=training,size=5,linout=TRUE,maxit=1500)
  pearson_cor<-cor.test(training$AbunVir,trained_net$fitted.values,method="pearson") 
  rmse<-rmse(training$AbunVir,trained_net$fitted.values)
  df_sample_ammount_check[i,] <- c(i, partition_size[[i]], pearson_cor$estimate, rmse)
}

ggplot(data = df_sample_ammount_check, aes(x = Fraction, y = PCC)) + 
  geom_line(aes(color = "PCC")) +
  geom_line(aes(y = RMSE, color = "RMSE")) +
  #geom_hline(yintercept=0.8396903, linetype = "dashed" ) +
  #geom_vline(xintercept=1500, linetype = "dashed" ) +
  scale_y_continuous(sec.axis = sec_axis(~., name = "RMSE")) +
  scale_color_manual(name = "Parametres", values = c("PCC" = "#26306e", "RMSE" = "#ffc701")) +
  ggtitle(paste("Checking the partition percentage used as a training set in my ANN"))
```

#### 3.3.5. Checking the decay in my ANN

```{r}
df_decay_check<- data.frame(decay= numeric(), PCC = numeric(), RMSE = numeric())

decay <- runif(100, min = 0, max = 1)

for (i in 1:length(decay)) {
  print(paste("Decay: ", decay[[i]]))
  trained_net<-nnet(AbunVir ~. ,data=df.3,size=4,linout=TRUE,maxit=1500,decay=decay[i])
  pearson_cor<-cor.test(df.3$AbunVir,trained_net$fitted.values,method="pearson") 
  rmse<-rmse(training$AbunVir,trained_net$fitted.values)
  df_decay_check[i,] <- c(decay[i], pearson_cor$estimate, rmse)
}

ggplot(data = df_decay_check, aes(x = decay, y = PCC)) + 
  geom_line(aes(color = "PCC")) +
  geom_line(aes(y = RMSE, color = "RMSE")) +
  #geom_hline(yintercept=0.8396903, linetype = "dashed" ) +
  #geom_vline(xintercept=1500, linetype = "dashed" ) +
  scale_y_continuous(sec.axis = sec_axis(~., name = "RMSE")) +
  scale_color_manual(name = "Parametres", values = c("PCC" = "#26306e", "RMSE" = "#ffc701")) +
  ggtitle(paste("Checking the decay in my ANN"))
```

## 3.4. Training my Pilot Single Layer ANN

We will use the "nnet" function from the "nnet" package to fit a single layer ANN model to the viral abundance data. We will use the "AbunVir" variable as the response variable and the remaining variables as the predictor variables. We will use the "linout=TRUE" argument to specify that the output layer should be linear. We will also use the "maxit" argument to specify the maximum number of iterations for the training process.

The steps to train the model are as follows: train-test split:

```         
1.  Do the train-test split --> Partitioning the data into two, test and train with a 75:25 ratio. 

2.  Fit the model to the train set 

3.  Test the model on the test set

4.  Calculate the prediction error 

5.  Repeat the process K times 

6.  Olden's importance test 
```

#### - Do the train-test split

createDataPartition() function from the caret package to split the original dataset into a training and testing set and split data into training (80%) and testing set (20%). It makes a random division but with the two sets of data balanced (that have similar data distributions).

```{r}
partition_num=0.8
train_set_row_nums<-createDataPartition(df.3$AbunVir,p=partition_num,list=FALSE)

# for training the model
training<-df.3[train_set_row_nums,] 
dim(training)
summary(training)
# for validating the model
testing<-df.3[-train_set_row_nums,] 
dim(testing)
summary(testing)
```

We check if our dta partition is evenly distributed = Balanced. We can use the Wilcoxon test to check if the distribution of the response variable is the same in the training and testing sets. The closer the p-value to 1 the better. Also histograms are plotted to check the distribution of the response variable in the training and testing sets.

```{r}
wilcox.test(training$AbunVir,testing$AbunVir,paired=FALSE,exact=FALSE) 

# Create a histogram
train_hist<- ggplot(training, aes(x = AbunVir)) + 
  geom_histogram(binwidth = 2, colour = "black", fill = "white") + 
  ggtitle(paste("Training: Histogram of distribution of the ", partition_num, "of the data"))
test_hist<- ggplot(testing, aes(x = AbunVir)) + 
  geom_histogram(binwidth = 2, colour = "black", fill = "white")  +
  ggtitle(paste("Testing: Histogram of distribution of the", 1-partition_num, "of the data"))

grid.arrange(train_hist, test_hist, ncol=2)
```

#### - Fit the model to the train set

Here we fit the model to the training set using the nnet function from the nnet package.

```{r warning=FALSE}
set.seed(666)
trained_net_TRAIN<-nnet(AbunVir ~ Salinity+Temperature+PO4+CHL+NO3+Abun_PNF ,data=df_imp_zscale,size=4,linout=TRUE,maxit=1500) 
pearson_cor_t_2<-cor.test(df_imp_zscale$AbunVir,trained_net_TRAIN$fitted.values, method="pearson")

rmse_t_2<-rmse(df_imp_zscale$AbunVir,trained_net_TRAIN$fitted.values)
pearson_cor_t_2
rmse_t_2

plot(trained_net_TRAIN)
```

#### - Test the model on the test set

We use the predict function to get the predictions for the validation set.

```{r warning=FALSE}
#Get predictions for the validation set
valid_preds<-predict(trained_net_TRAIN,testing,type=c("raw"))
summary(valid_preds) #vector of the predictions on our valdiation set
```

#### - Calculate the prediction error (Repeat the whole process K times)

What we want to see is how well our model performs on the validation set. We will use the correlation coefficient and the root mean square error (RMSE) to evaluate the performance of the model on the validation set. The best model should have a high correlation coefficient and a low RMSE.

```{r warning=FALSE}
#Evaluate the performance of the new model on the validation set
cor.test(testing$AbunVir,valid_preds)
rmse(testing$AbunVir,valid_preds)
```

#### - OLDEN'S IMPORTANCE TEST

The **Olden method** is a method for calculating the importance of predictors in a neural network model. It is based on the idea that the importance of a predictor can be measured by the change in the output of the model when the predictor is removed. The Olden method calculates the importance of each predictor by removing it from the model and measuring the change in the output. The predictors with the largest change in output are considered the most important.

```{r}
set.seed(666)
#Check the contents of the output ANN object
ls(trained_net_TRAIN)
plot(best_model)
#Calculate the importance of the predictors using the Olden method
importance_olden<-olden(trained_net_TRAIN,bar_plot=T) 

gar.fun('Virus',best_model)

#Look at the outpt and identify the most important predictor
olden(trained_net_TRAIN,bar_plot=TRUE) 
```

In addition we use the **lekprofile function** to visualize the relative importance of the predictors in the model. The lekprofile function creates a profile plot that shows the relative importance of each predictor in the model. The profile plot can be used to identify the most important predictors and to compare the relative importance of different predictors.

```{r}
lekprofile(trained_net_TRAIN)
```

```{r eval=FALSE, include=FALSE}

#import the function from Github
#library(devtools)
source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
 
#plot each model
plot(trained_net_TRAIN, main="Neural networks with a single hidden layer: Training set")

#relative importance function
#library(devtools)
source_url('https://gist.github.com/fawda123/6206737/raw/2e1bc9cbc48d1a56d2a79dd1d33f414213f5f1b1/gar_fun.r')
num.vars<-ncol(df.3)
#relative importance of input variables for AbunVir
rel.imp<-gar.fun('AbunVir',trained_net,bar.plot=T)$rel.imp
 
#color vector based on relative importance of input values
cols<-colorRampPalette(c('red','green'))(num.vars)[rank(rel.imp)]
 
##
#plotting function
#source_url('https://gist.githubusercontent.com/fawda123/7471137/raw/466c1474d0a505ff044412703516c34f1a4684a5/nnet_plot_update.r')
  
#plot model with new color vector
#separate colo rs for input vectors using a list for 'circle.col'
plot(best_model,circle.col=list(cols,'lightblue'))
```

## 3.5. Identification and Evaluation of the Best Neural Network Model for Predicting the Response Variable

This code implements an exhaustive search process to identify the best neural network model for predicting a response variable in a dataset. Various combinations of predictor variables, such as salinity, temperature, and others, are explored using the neural network function from the nnet library in R. The performance of each model is evaluated through correlation and root mean square error (RMSE) on a test set. The code tracks the iteration where the best model is found, providing information about the optimal combination of predictor variables and its performance on both test and training sets. The ultimate goal is to generate an accurate model for predicting the response variable based on the selected predictor variables.

```{r}
df_imp <- read_csv("ANN_data_impotation_n_is_246.csv")
df_imp_zscale <- as.data.frame(scale(df_imp, center = T))
attach(df_imp_zscale)
```

```{r}
# Dividir los datos en conjuntos de entrenamiento y prueba
set.seed(666)  # Fijar una semilla para reproducibilidad
train_indices <- createDataPartition(df_imp_zscale$AbunVir, p = 0.8, list = FALSE)  # 80% de los datos para entrenamiento
train_data <- df_imp_zscale[train_indices, ]
test_data <- df_imp_zscale[-train_indices, ]

# Variables disponibles
#variables <- c("Salinity","Temperature", "Secchi_Disk", "PO4", "Abun_HNF", "CHL", "NO3", "Abun_PNF", "Synechococcus", "Proclorococcus")

variables <- c("Temperature","Salinity", "PO4", "NO3", "CHL", "Abun_PNF", "AbunBact") 
#bacteria variables <- c("Synechococcus", "Proclorococcus", "Abun_PNF", "Abun_HNF",  "NO2",  "SiO4", "PO4")
#HNF variables <- c("Synechococcus", "Proclorococcus", "CHL", "NO2", "NO3", "SiO4", "AbunBact")
#PNF variables <- c("Salinity", "Temperature", "Proclorococcus", "PO4", "CHL",  "SiO4", "AbunBact")

# Inicializar valores de correlación y RMSE
results_df <- data.frame(variables = character(), iteration_num = integer(), num_variables= integer(), corr_train = numeric(), rmse_train = numeric(), corr_test = numeric(), rmse_test = numeric(), best_all_data_corr = numeric(), best_all_data_rmse = numeric(), stringsAsFactors = FALSE)

best_corr <- 0
best_rmse <- Inf
best_combination <- NULL
best_model <- NULL
best_corr_train <- 0
best_rmse_train <- Inf
model_count <- 0
best_iteration <- 0

# Bucle para evaluar cada combinación
#for (i in length(variables)) {
  # Obtener todas las combinaciones posibles de longitud i con AbunVir incluida
  combinations <- combn(variables, 5, simplify = FALSE)
  
  for (j in 1:length(combinations)) {
    combination <- combinations[[j]]
    # Construir fórmula para el modelo

    formula <- as.formula(paste("AbunVir ~", paste(combination, collapse = "+")))
    
    # Ajustar modelo de red neuronal a los datos de entrenamiento
    set.seed(666)
    model <- nnet::nnet(formula, data = train_data, size = 3, linout = TRUE, maxit = 1500)
    model_count <- model_count + 1
    
    # Predecir los valores de AbunVir para los datos de prueba
    predictions <- predict(model, newdata = test_data, inf.rm = TRUE, na.rm = TRUE, nan.rm = TRUE)
    
    # Calcular correlación y RMSE en los datos de prueba
    corr <- cor(predictions, test_data$AbunVir)
    rmse <- sqrt(mean((predictions - test_data$AbunVir)^2))
    
    # Predecir los valores de AbunVir para los datos de entrenamiento
    predictions_train <- predict(model, newdata = train_data, inf.rm = TRUE, na.rm = TRUE, nan.rm = TRUE)
    
    # Calcular correlación y RMSE en los datos de entrenamiento
    corr_train <- cor(predictions_train, train_data$AbunVir)
    rmse_train <- sqrt(mean((predictions_train - train_data$AbunVir)^2))
    
    
    # Construir fórmula para el modelo con ALL DATA
    all_data_model <- nnet::nnet(formula, data = df_imp_zscale, size = 3, linout = TRUE, maxit = 1500)

    # Predecir los valores de AbunVir para todos los datos
    all_data_predictions <- predict(all_data_model, newdata = df_imp_zscale, inf.rm = TRUE, na.rm = TRUE, nan.rm = TRUE)

    # Calcular correlación y RMSE en todos los datos
    all_data_best_corr <- cor(all_data_predictions, df_imp_zscale$AbunVir)
    all_data_best_rmse <- sqrt(mean((all_data_predictions - df_imp_zscale$AbunVir)^2))
    
    
    # Actualizar la mejor combinación si se cumple el criterio de selección
    if (corr > best_corr || (corr == best_corr && rmse < best_rmse)) {
      best_corr <- corr
      best_rmse <- rmse
      best_combination <- combination
      best_model <- model
      best_corr_train <- corr_train
      best_rmse_train <- rmse_train
      best_iteration <- j
      best_all_data_corr <- all_data_best_corr
      best_all_data_rmse <- all_data_best_rmse
      
    }
  # Almacenar resultados utilizando rbind
    result_row <- data.frame(variables = paste(combination, collapse = ", "), iteration_num = model_count, num_variables = length(combination), corr_train = corr_train, rmse_train = rmse_train, corr_test = corr, rmse_test = rmse, all_data_corr = best_all_data_corr, all_data_rmse = best_all_data_rmse,  stringsAsFactors = FALSE)
    results_df <- rbind(results_df, result_row)
  }
#}

# Imprimir resultados
cat("Total de modelos evaluados:", model_count, "\n")
cat("Mejor combinación de variables en la iteración:", best_iteration, "\n")
cat("Mejor combinación de variables:", paste(best_combination, collapse = ", "), "\n")
cat("Correlación en datos de prueba:", best_corr, "\n")
cat("RMSE en datos de prueba:", best_rmse, "\n")
cat("Correlación en datos de entrenamiento:", best_corr_train, "\n")
cat("RMSE en datos de entrenamiento:", best_rmse_train, "\n")
# Imprimir resultados
cat("Mejor combinación de variables:", paste(best_combination, collapse = ", "), "\n")
cat("Correlación en todos los datos:", best_all_data_corr, "\n")
cat("RMSE en todos los datos:", best_all_data_rmse, "\n") 

# Plot del modelo
plot(best_model)

```

```{r}
# Construir fórmula para el modelo con la mejor combinación
formula <- as.formula(paste("AbunVir ~", paste(best_combination, collapse = "+")))

# Ajustar modelo de red neuronal a todos los datos
best_model <- nnet::nnet(formula, data = df_imp_zscale, size = 4, linout = TRUE, maxit = 1500)

# Predecir los valores de AbunVir para todos los datos
predictions <- predict(best_model, newdata = df_imp_zscale, inf.rm = TRUE, na.rm = TRUE, nan.rm = TRUE)


# Calcular correlación y RMSE en todos los datos
best_corr <- cor(predictions, df_imp_zscale$AbunVir)
best_rmse <- sqrt(mean((predictions - df_imp_zscale$AbunVir)^2))

# Imprimir resultados
cat("Mejor combinación de variables:", paste(best_combination, collapse = ", "), "\n")
cat("Correlación en todos los datos:", best_corr, "\n")
cat("RMSE en todos los datos:", best_rmse, "\n")
```

## 3.5. References

-   [Basics of Neural Networks: How many neurons I need?](https://www.yourdatateacher.com/2021/05/10/how-many-neurons-for-a-neural-network/) Este es un buen link para entender el número de neuronas necesarias en una red neuronal
-   [Artificial Neural Network using R Studio](https://medium.com/@sukmaanindita/artificial-neural-network-using-r-studio-3eb538fa39fb)
-   [Visualizing neural networks in R](https://beckmw.wordpress.com/2013/11/14/visualizing-neural-networks-in-r-update/)
-   [How to tune hyperparameters with R](https://www.projectpro.io/recipes/tune-hyper-parameters-grid-search-r)
-   [Tuning hyperparameters in a neural network](https://f0nzie.github.io/machine_learning_compilation/tuning-hyperparameters-in-a-neural-network.html)
